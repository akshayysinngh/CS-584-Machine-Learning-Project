{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c3b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57fbbade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "happy\n"
     ]
    }
   ],
   "source": [
    "#emotion_detection.py\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import numpy as np  #this will be used later in the process\n",
    "\n",
    "imgpath = \"sad face.jpg\"  #put the image where this file is located and put its name here\n",
    "image = cv2.imread(imgpath)\n",
    "\n",
    "analyze = DeepFace.analyze(img_path = \"emtion detection sample.jpg\", \n",
    "        actions = ['emotion']\n",
    ")\n",
    "print(analyze['dominant_emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104682a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c230778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: age:   0%|                                                                               | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 385ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender:  25%|█████████████████                                                   | 1/4 [00:00<00:01,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 383ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race:  50%|███████████████████████████████████                                   | 2/4 [00:00<00:00,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 343ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion:  75%|██████████████████████████████████████████████████▎                | 3/4 [00:01<00:00,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|███████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': 24, 'region': {'x': 15, 'y': 77, 'w': 151, 'h': 151}, 'gender': 'Man', 'race': {'asian': 8.427254042144304e-18, 'indian': 3.897221036690138e-13, 'black': 100.0, 'white': 7.495560955076496e-24, 'middle eastern': 1.863529621596095e-25, 'latino hispanic': 3.3714723714803834e-16}, 'dominant_race': 'black', 'emotion': {'angry': 0.006327968003461137, 'disgust': 4.3870915233945595e-08, 'fear': 0.3449042793363333, 'happy': 99.29653406143188, 'sad': 0.006079572631279007, 'surprise': 0.30677865725010633, 'neutral': 0.03937420260626823}, 'dominant_emotion': 'happy'}\n"
     ]
    }
   ],
   "source": [
    "obj = DeepFace.analyze(img_path = \"emtion detection sample.jpg\", \n",
    "        actions = ['age', 'gender', 'race', 'emotion']\n",
    ")\n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee1b478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821d2657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef11bc24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca8463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d8d0394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 433ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 360ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 409ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 321ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "fear\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "fear\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "surprise\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "surprise\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "sad\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "surprise\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "surprise\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "fear\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "sad\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "sad\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 295ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 244ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 294ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "happy\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "\n",
    "face_cascade_name = cv2.data.haarcascades + 'haarcascade_frontalface_alt.xml'  #getting a haarcascade xml file\n",
    "face_cascade = cv2.CascadeClassifier()  #processing it for our project\n",
    "if not face_cascade.load(cv2.samples.findFile(face_cascade_name)):  #adding a fallback event\n",
    "    print(\"Error loading xml file\")\n",
    "\n",
    "video=cv2.VideoCapture(0)  #requisting the input from the webcam or camera\n",
    "\n",
    "while True:  \n",
    "    ret,frame = video.read()\n",
    "\n",
    "    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)  #changing the video to grayscale to make the face analisis work properly\n",
    "    face=face_cascade.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5)\n",
    "\n",
    "    for x,y,w,h in face:\n",
    "        img = cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255),1)  #making a recentangle to show up and detect the face and setting it position and colour\n",
    "   \n",
    "      #making a try and except condition in case of any errors\n",
    "        try:\n",
    "            analyze = DeepFace.analyze(frame, actions = ['emotion'])\n",
    "          #analyze = DeepFace.analyze(frame,actions=['emotions'])  #same thing is happing here as the previous example, we are using the analyze class from deepface and using ‘frame’ as input\n",
    "            print(analyze['dominant_emotion'])  #here we will only go print out the dominant emotion also explained in the previous example\n",
    "        except:\n",
    "            print(\"no face\")\n",
    "\n",
    "      #this is the part where we display the output to the user\n",
    "      \n",
    "    cv2.imshow('frame', frame)\n",
    "      \n",
    "    key=cv2.waitKey(1) & 0xFF\n",
    "    if key==ord('q'):# here we are specifying the key which will stop the loop and stop all the processes going\n",
    "        cv2.destroyAllWindows()\n",
    "        video.release()\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9fc215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0235c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras.preprocessing.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "738c66e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'img_to_array' from 'keras.preprocessing.image' (C:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\preprocessing\\image.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m img_to_array\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimutils\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'img_to_array' from 'keras.preprocessing.image' (C:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\preprocessing\\image.py)"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "import imutils\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# parameters for loading data and images\n",
    "detection_model_path = 'haarcascade_files/haarcascade_frontalface_default.xml'\n",
    "emotion_model_path = 'models/_mini_XCEPTION.102-0.66.hdf5'\n",
    "\n",
    "# hyper-parameters for bounding boxes shape\n",
    "# loading models\n",
    "face_detection = cv2.CascadeClassifier(detection_model_path)\n",
    "emotion_classifier = load_model(emotion_model_path, compile=False)\n",
    "EMOTIONS = [\"angry\" ,\"disgust\",\"scared\", \"happy\", \"sad\", \"surprised\",\n",
    " \"neutral\"]\n",
    "\n",
    "\n",
    "#feelings_faces = []\n",
    "#for index, emotion in enumerate(EMOTIONS):\n",
    "   # feelings_faces.append(cv2.imread('emojis/' + emotion + '.png', -1))\n",
    "\n",
    "# starting video streaming\n",
    "cv2.namedWindow('your_face')\n",
    "camera = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    frame = camera.read()[1]\n",
    "    #reading the frame\n",
    "    frame = imutils.resize(frame,width=300)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detection.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    \n",
    "    canvas = np.zeros((250, 300, 3), dtype=\"uint8\")\n",
    "    frameClone = frame.copy()\n",
    "    if len(faces) > 0:\n",
    "        faces = sorted(faces, reverse=True,\n",
    "        key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))[0]\n",
    "        (fX, fY, fW, fH) = faces\n",
    "                    # Extract the ROI of the face from the grayscale image, resize it to a fixed 28x28 pixels, and then prepare\n",
    "            # the ROI for classification via the CNN\n",
    "        roi = gray[fY:fY + fH, fX:fX + fW]\n",
    "        roi = cv2.resize(roi, (64, 64))\n",
    "        roi = roi.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "        \n",
    "        \n",
    "        preds = emotion_classifier.predict(roi)[0]\n",
    "        emotion_probability = np.max(preds)\n",
    "        label = EMOTIONS[preds.argmax()]\n",
    "    else: continue\n",
    "\n",
    " \n",
    "    for (i, (emotion, prob)) in enumerate(zip(EMOTIONS, preds)):\n",
    "                # construct the label text\n",
    "                text = \"{}: {:.2f}%\".format(emotion, prob * 100)\n",
    "\n",
    "                # draw the label + probability bar on the canvas\n",
    "               # emoji_face = feelings_faces[np.argmax(preds)]\n",
    "\n",
    "                \n",
    "                w = int(prob * 300)\n",
    "                cv2.rectangle(canvas, (7, (i * 35) + 5),\n",
    "                (w, (i * 35) + 35), (0, 0, 255), -1)\n",
    "                cv2.putText(canvas, text, (10, (i * 35) + 23),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.45,\n",
    "                (255, 255, 255), 2)\n",
    "                cv2.putText(frameClone, label, (fX, fY - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "                cv2.rectangle(frameClone, (fX, fY), (fX + fW, fY + fH),\n",
    "                              (0, 0, 255), 2)\n",
    "#    for c in range(0, 3):\n",
    "#        frame[200:320, 10:130, c] = emoji_face[:, :, c] * \\\n",
    "#        (emoji_face[:, :, 3] / 255.0) + frame[200:320,\n",
    "#        10:130, c] * (1.0 - emoji_face[:, :, 3] / 255.0)\n",
    "\n",
    "\n",
    "    cv2.imshow('your_face', frameClone)\n",
    "    cv2.imshow(\"Probabilities\", canvas)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b347ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe662ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5307e317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebe731be",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'imread' from 'scipy.misc' (C:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\misc\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [27], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_offsets\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_detection_model\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocess_input\n\u001b[0;32m     13\u001b[0m USE_WEBCAM \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m# If false, loads video file source\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# parameters for loading data and images\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\Jupyter notebooks\\Drowsiness detection code - 1\\utils\\preprocessor.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m imread, imresize\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_input\u001b[39m(x, v2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m      6\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'imread' from 'scipy.misc' (C:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\misc\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from statistics import mode\n",
    "from utils.datasets import get_labels\n",
    "from utils.inference import detect_faces\n",
    "from utils.inference import draw_text\n",
    "from utils.inference import draw_bounding_box\n",
    "from utils.inference import apply_offsets\n",
    "from utils.inference import load_detection_model\n",
    "from utils.preprocessor import preprocess_input\n",
    "\n",
    "USE_WEBCAM = True # If false, loads video file source\n",
    "\n",
    "# parameters for loading data and images\n",
    "emotion_model_path = './models/emotion_model.hdf5'\n",
    "emotion_labels = get_labels('fer2013')\n",
    "\n",
    "# hyper-parameters for bounding boxes shape\n",
    "frame_window = 10\n",
    "emotion_offsets = (20, 40)\n",
    "\n",
    "# loading models\n",
    "face_cascade = cv2.CascadeClassifier('./models/haarcascade_frontalface_default.xml')\n",
    "emotion_classifier = load_model(emotion_model_path)\n",
    "\n",
    "# getting input model shapes for inference\n",
    "emotion_target_size = emotion_classifier.input_shape[1:3]\n",
    "\n",
    "# starting lists for calculating modes\n",
    "emotion_window = []\n",
    "\n",
    "# starting video streaming\n",
    "\n",
    "cv2.namedWindow('window_frame')\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Select video or webcam feed\n",
    "cap = None\n",
    "if (USE_WEBCAM == True):\n",
    "    cap = cv2.VideoCapture(0) # Webcam source\n",
    "else:\n",
    "    cap = cv2.VideoCapture('./demo/dinner.mp4') # Video file source\n",
    "\n",
    "while cap.isOpened(): # True:\n",
    "    ret, bgr_image = cap.read()\n",
    "\n",
    "    #bgr_image = video_capture.read()[1]\n",
    "\n",
    "    gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n",
    "    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5,\n",
    "\t\t\tminSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "    for face_coordinates in faces:\n",
    "\n",
    "        x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n",
    "        gray_face = gray_image[y1:y2, x1:x2]\n",
    "        try:\n",
    "            gray_face = cv2.resize(gray_face, (emotion_target_size))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        gray_face = preprocess_input(gray_face, True)\n",
    "        gray_face = np.expand_dims(gray_face, 0)\n",
    "        gray_face = np.expand_dims(gray_face, -1)\n",
    "        emotion_prediction = emotion_classifier.predict(gray_face)\n",
    "        emotion_probability = np.max(emotion_prediction)\n",
    "        emotion_label_arg = np.argmax(emotion_prediction)\n",
    "        emotion_text = emotion_labels[emotion_label_arg]\n",
    "        emotion_window.append(emotion_text)\n",
    "\n",
    "        if len(emotion_window) > frame_window:\n",
    "            emotion_window.pop(0)\n",
    "        try:\n",
    "            emotion_mode = mode(emotion_window)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if emotion_text == 'angry':\n",
    "            color = emotion_probability * np.asarray((255, 0, 0))\n",
    "        elif emotion_text == 'sad':\n",
    "            color = emotion_probability * np.asarray((0, 0, 255))\n",
    "        elif emotion_text == 'happy':\n",
    "            color = emotion_probability * np.asarray((255, 255, 0))\n",
    "        elif emotion_text == 'surprise':\n",
    "            color = emotion_probability * np.asarray((0, 255, 255))\n",
    "        else:\n",
    "            color = emotion_probability * np.asarray((0, 255, 0))\n",
    "\n",
    "        color = color.astype(int)\n",
    "        color = color.tolist()\n",
    "\n",
    "        draw_bounding_box(face_coordinates, rgb_image, color)\n",
    "        draw_text(face_coordinates, rgb_image, emotion_mode,\n",
    "                  color, 0, -45, 1, 1)\n",
    "\n",
    "    bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imshow('window_frame', bgr_image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce2e6da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyaudio\n",
      "  Downloading PyAudio-0.2.12-cp310-cp310-win_amd64.whl (163 kB)\n",
      "     -------------------------------------- 164.0/164.0 kB 9.6 MB/s eta 0:00:00\n",
      "Installing collected packages: pyaudio\n",
      "Successfully installed pyaudio-0.2.12\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e2da4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01ddbb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fer\n",
      "  Downloading fer-22.4.0-py3-none-any.whl (812 kB)\n",
      "     ------------------------------------- 812.1/812.1 kB 10.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fer) (3.6.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fer) (4.64.1)\n",
      "Requirement already satisfied: keras>=2.0.0 in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fer) (2.10.0)\n",
      "Requirement already satisfied: mtcnn>=0.1.1 in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fer) (0.1.1)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\rahul\\appdata\\roaming\\python\\python310\\site-packages (from fer) (4.6.0.66)\n",
      "Requirement already satisfied: requests in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fer) (2.28.1)\n",
      "Requirement already satisfied: opencv-python>=4.1.0 in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mtcnn>=0.1.1->fer) (4.6.0.66)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fer) (4.38.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fer) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fer) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fer) (1.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fer) (21.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fer) (1.0.6)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fer) (9.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fer) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fer) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->fer) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->fer) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->fer) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->fer) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->fer) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->fer) (1.16.0)\n",
      "Installing collected packages: fer\n",
      "Successfully installed fer-22.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install fer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "846ed492",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Video file not found at C:\\content\\Video_One.mp4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [28], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m face_detector \u001b[38;5;241m=\u001b[39m FER(mtcnn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Input the video for processing\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m input_video \u001b[38;5;241m=\u001b[39m \u001b[43mVideo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation_videofile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# The Analyze() function will run analysis on every frame of the input video. \u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# It will create a rectangular box around every image and show the emotion values next to that.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Finally, the method will publish a new video that will have a box around the face of the human with live emotion values.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m processing_data \u001b[38;5;241m=\u001b[39m input_video\u001b[38;5;241m.\u001b[39manalyze(face_detector, display\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\fer\\classes.py:45\u001b[0m, in \u001b[0;36mVideo.__init__\u001b[1;34m(self, video_file, outdir, first_face_only, tempfile)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     33\u001b[0m     video_file: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     tempfile: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     37\u001b[0m ):\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124;03m\"\"\"Video class for extracting and saving frames for emotion detection.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    :param video_file - str\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    :param outdir - str\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    :param tempfile - str\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(video_file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo file not found at \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     46\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(video_file)\n\u001b[0;32m     47\u001b[0m     )\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(video_file)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(outdir):\n",
      "\u001b[1;31mAssertionError\u001b[0m: Video file not found at C:\\content\\Video_One.mp4"
     ]
    }
   ],
   "source": [
    "from fer import Video\n",
    "from fer import FER\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Put in the location of the video file that has to be processed\n",
    "location_videofile = \"/content/Video_One.mp4\"\n",
    "\n",
    "# Build the Face detection detector\n",
    "face_detector = FER(mtcnn=True)\n",
    "# Input the video for processing\n",
    "input_video = Video(location_videofile)\n",
    "\n",
    "# The Analyze() function will run analysis on every frame of the input video. \n",
    "# It will create a rectangular box around every image and show the emotion values next to that.\n",
    "# Finally, the method will publish a new video that will have a box around the face of the human with live emotion values.\n",
    "processing_data = input_video.analyze(face_detector, display=False)\n",
    "\n",
    "# We will now convert the analysed information into a dataframe.\n",
    "# This will help us import the data as a .CSV file to perform analysis over it later\n",
    "vid_df = input_video.to_pandas(processing_data)\n",
    "vid_df = input_video.get_first_face(vid_df)\n",
    "vid_df = input_video.get_emotions(vid_df)\n",
    "\n",
    "# Plotting the emotions against time in the video\n",
    "pltfig = vid_df.plot(figsize=(20, 8), fontsize=16).get_figure()\n",
    "\n",
    "# We will now work on the dataframe to extract which emotion was prominent in the video\n",
    "angry = sum(vid_df.angry)\n",
    "disgust = sum(vid_df.disgust)\n",
    "fear = sum(vid_df.fear)\n",
    "happy = sum(vid_df.happy)\n",
    "sad = sum(vid_df.sad)\n",
    "surprise = sum(vid_df.surprise)\n",
    "neutral = sum(vid_df.neutral)\n",
    "\n",
    "emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "emotions_values = [angry, disgust, fear, happy, sad, surprise, neutral]\n",
    "\n",
    "score_comparisons = pd.DataFrame(emotions, columns = ['Human Emotions'])\n",
    "score_comparisons['Emotion Value from the Video'] = emotions_values\n",
    "score_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d7331a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
